{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"crawler_github.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNm8n/gvEWsqj+Kd037gTKt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5xLV3iMQZtFD","colab_type":"text"},"source":["##Crawl the arXiv papers"]},{"cell_type":"code","metadata":{"id":"_DsKXhX_VumI","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","import urllib\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmdeGVpIzXor","colab_type":"code","colab":{}},"source":["def crawl_page(field, yymm, skip, label_dict):\n","    url = f'https://arxiv.org/list/{field}/{yymm}?skip={skip}&show=1000'\n","\n","    with urllib.request.urlopen(url) as respond:\n","        html = respond.read()\n","        soup = BeautifulSoup(html, 'html.parser')\n","\n","    titles = soup.find_all('div', {'class': 'list-title'})\n","    paper_urls = soup.find_all('span', {'class': 'list-identifier'})\n","\n","    papers = []\n","    for i in range(len(titles)):\n","        paper = []\n","        title = titles[i].contents[-1].strip()\n","        paper.append(title)\n","\n","        paper_url = 'https://arxiv.org' + paper_urls[i].find_all('a')[0].attrs['href']\n","        headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n","        req = urllib.request.Request(paper_url, headers=headers)\n","        try:\n","            respond = urllib.request.urlopen(req)\n","        except urllib.error.HTTPError as e:\n","            print(f'HTTPError occured: {i}-th paper of skip={skip}&show=1000')\n","            continue\n","\n","        html = respond.read()\n","        soup = BeautifulSoup(html, 'html.parser')\n","        \n","        abstract = soup.find('meta', {'property': 'og:description'}).attrs['content']\n","        paper.append(abstract)\n","        \n","        subject_list = soup.find('td', {'class': 'tablecell subjects'}).text.split(';')\n","        for subject in subject_list:\n","            subject = subject.split('(')[-1].split(')')[0]\n","            if subject in label_dict:\n","                paper.append(label_dict.get(subject))\n","                break\n","        if subject not in label_dict:\n","            continue\n","        \n","        papers.append(paper)\n","    \n","    dataframe = pd.DataFrame(papers)\n","    dataframe.to_csv(os.path.join(cwd, f'data/arxiv/{field}/{yymm}-{skip}.csv'),\n","                     header=['title', 'abstract', 'subject'],\n","                     index=False,\n","                     mode='w')\n","    \n","    print(f'Successfully written: {field}/{yymm}-{skip}.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DN1zgMZU4iNQ","colab_type":"code","colab":{}},"source":["drive.mount('/gdrive', force_remount=True)\n","cwd = '/gdrive/My Drive/paper-classification'\n","\n","field = 'cs'\n","yymm = 1910\n","skip = 0\n","\n","with open(os.path.join(cwd, f'data/label/{field}.subject'), 'r') as f:\n","    label_list = f.read().split()\n","    label_dict = {s:i for i, s in enumerate(label_list)}\n","\n","crawl_page(field, yymm, skip, label_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qUfhXdJjZzDf","colab_type":"text"},"source":["##Concat csv files (same yymm)"]},{"cell_type":"code","metadata":{"id":"Lv2ExWg06_qa","colab_type":"code","outputId":"728a0e33-83c4-46d9-9bb3-5e4da9f0abdd","executionInfo":{"status":"ok","timestamp":1588189215687,"user_tz":-540,"elapsed":882,"user":{"displayName":"Subin Jeon","photoUrl":"","userId":"15652485623875898558"}},"colab":{"base_uri":"https://localhost:8080/","height":56}},"source":["import glob\n","filelist = sorted(glob.glob(os.path.join(cwd, f'data/arxiv/{field}/{yymm}-*')))\n","print(filelist)\n","datalist = [pd.read_csv(f) for f in filelist]\n","dataframe = pd.concat(datalist, axis=0, ignore_index=False)\n","dataframe.to_csv(os.path.join(cwd, f'data/arxiv/{field}-{yymm}.csv'),\n","                 index=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['/gdrive/My Drive/paper-classification/data/arxiv/new/1911-0.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/1911-1000.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/1911-2000.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/1911-3000.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/1911-4000.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/1911-5000.csv']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"91FfO-ffd89c","colab_type":"text"},"source":["##Concat csv files (same field)"]},{"cell_type":"code","metadata":{"id":"zBnjM_jkkpeM","colab_type":"code","outputId":"2823dc3c-4bd0-4f58-8ac7-2e1d802af550","executionInfo":{"status":"ok","timestamp":1588189609117,"user_tz":-540,"elapsed":639,"user":{"displayName":"Subin Jeon","photoUrl":"","userId":"15652485623875898558"}},"colab":{"base_uri":"https://localhost:8080/","height":56}},"source":["import glob\n","filelist = sorted(glob.glob(os.path.join(cwd, f'data/arxiv/{field}/*')))\n","print(filelist)\n","datalist = [pd.read_csv(f) for f in filelist]\n","dataframe = pd.concat(datalist, axis=0, ignore_index=False)\n","dataframe.to_csv(os.path.join(cwd, f'data/arxiv/{field}/data.csv'),\n","                 index=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['/gdrive/My Drive/paper-classification/data/arxiv/new/1910.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/1911.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/1912.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/2001.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/2002.csv', '/gdrive/My Drive/paper-classification/data/arxiv/new/2003.csv']\n"],"name":"stdout"}]}]}