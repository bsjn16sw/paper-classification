{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xLV3iMQZtFD",
        "colab_type": "text"
      },
      "source": [
        "##Crawl the arXiv papers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DsKXhX_VumI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN1zgMZU4iNQ",
        "colab_type": "code",
        "outputId": "f8d22f54-5a8b-4152-e1cb-33ffe205b2e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "drive.mount('/gdrive', force_remount=True)\n",
        "cwd = '/gdrive/My Drive/paper-classification'\n",
        "with open(os.path.join(cwd, 'data/label/cs.subject'), 'r') as f:\n",
        "    label_list = f.read().split()\n",
        "    label_dict = {s:i for i, s in enumerate(label_list)}\n",
        "print(label_dict)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "{'cs.AI': 0, 'cs.CL': 1, 'cs.CC': 2, 'cs.CE': 3, 'cs.C': 4, 'cs.GT': 5, 'cs.CV': 6, 'cs.CY': 7, 'cs.CR': 8, 'cs.DS': 9, 'cs.DB': 10, 'cs.DL': 11, 'cs.DM': 12, 'cs.DC': 13, 'cs.ET': 14, 'cs.FL': 15, 'cs.GL': 16, 'cs.GR': 17, 'cs.AR': 18, 'cs.HC': 19, 'cs.IR': 20, 'cs.IT': 21, 'cs.LO': 22, 'cs.LG': 23, 'cs.MS': 24, 'cs.MA': 25, 'cs.MM': 26, 'cs.NI': 27, 'cs.NE': 28, 'cs.NA': 29, 'cs.OS': 30, 'cs.OH': 31, 'cs.PF': 32, 'cs.PL': 33, 'cs.RO': 34, 'cs.SI': 35, 'cs.SE': 36, 'cs.SD': 37, 'cs.SC': 38, 'cs.SY': 39}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmdeGVpIzXor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crawl_page(yymm, skip):\n",
        "    url = 'https://arxiv.org/list/cs/{}?skip={}&show=1000'.format(yymm, skip)\n",
        "\n",
        "    with urllib.request.urlopen(url) as respond:\n",
        "        html = respond.read()\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    titles = soup.find_all('div', {'class': 'list-title'})\n",
        "    paper_urls = soup.find_all('span', {'class': 'list-identifier'})\n",
        "\n",
        "    papers = []\n",
        "    for j in range(len(titles)):\n",
        "        paper = []\n",
        "        title = titles[j].contents[-1].strip()\n",
        "        paper.append(title)\n",
        "\n",
        "        paper_url = 'https://arxiv.org' + paper_urls[j].find_all('a')[0].attrs['href']\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
        "                   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "                   'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "                   'Accept-Encoding': 'none',\n",
        "                   'Accept-Language': 'en-US,en;q=0.8',\n",
        "                   'Connection': 'keep-alive'}\n",
        "        req = urllib.request.Request(paper_url, headers=headers)\n",
        "        try:\n",
        "            respond = urllib.request.urlopen(req)\n",
        "        except urllib.error.HTTPError as e:\n",
        "            print('HTTPError occured in {}-th paper of {}-th page'.format(j, skip))\n",
        "            continue\n",
        "\n",
        "        html = respond.read()\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        \n",
        "        abstract = soup.find('meta', {'property': 'og:description'}).attrs['content']\n",
        "        paper.append(abstract)\n",
        "        \n",
        "        subject_list = soup.find('td', {'class': 'tablecell subjects'}).text.split(';')\n",
        "        for subject in subject_list:\n",
        "            subject = subject.split('(')[-1].split(')')[0]\n",
        "            if subject in label_dict:\n",
        "                paper.append(label_dict.get(subject))\n",
        "                break\n",
        "        if subject not in label_dict:\n",
        "            continue\n",
        "        \n",
        "        papers.append(paper)\n",
        "    \n",
        "    dataframe = pd.DataFrame(papers)\n",
        "    dataframe.to_csv(os.path.join(cwd, 'data/arxiv/{}-{}.csv'.format(yymm, skip)),\n",
        "                         header=['title', 'abstract', 'subject'],\n",
        "                         index=False,\n",
        "                         mode='w')\n",
        "    \n",
        "    print('Successfully written: {}-{}.csv'.format(yymm, skip))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yI6sdnF0BmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f966007e-ab49-42cf-deba-b9d0e7cbc77b"
      },
      "source": [
        "'''\n",
        "for i in range (0, 6000, 1000):\n",
        "    crawl_page(2003, i*1000)\n",
        "\n",
        "-> 이렇게 for-loop으로 돌리면 HTTPError가 계속적으로 발생함. (403 forbidden)\n",
        "-> 해결법을 구글링해보면 header를 붙이라고 나오나 붙여도 계속 발생함.\n",
        "-> multiprocessing을 사용해도 동일한 현상 발생함.\n",
        "-> 그냥 아래처럼 for-loop을 펼쳐서 한 페이지씩 크롤링하는 게 최선.\n",
        "-> 같은 ipynb 여러 개 만들어서 동시에 실행했을 때는 괜찮았음.\n",
        "'''\n",
        "yymm = 2002\n",
        "crawl_page(yymm, 0)\n",
        "#crawl_page(yymm, 1000)\n",
        "#crawl_page(yymm, 2000)\n",
        "#crawl_page(yymm, 3000)\n",
        "#crawl_page(yymm, 4000)\n",
        "#crawl_page(yymm, 5000)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully written: 2002-0.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUfhXdJjZzDf",
        "colab_type": "text"
      },
      "source": [
        "##Concat csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv2ExWg06_qa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "14cbb0fa-4728-424a-ceca-5616c9801545"
      },
      "source": [
        "import glob\n",
        "filelist = sorted(glob.glob(os.path.join(cwd, 'data/arxiv/{}-*'.format(yymm))))\n",
        "print(filelist)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/gdrive/My Drive/paper-classification/data/arxiv/2002-0.csv', '/gdrive/My Drive/paper-classification/data/arxiv/2002-1000.csv', '/gdrive/My Drive/paper-classification/data/arxiv/2002-2000.csv', '/gdrive/My Drive/paper-classification/data/arxiv/2002-3000.csv', '/gdrive/My Drive/paper-classification/data/arxiv/2002-4000.csv', '/gdrive/My Drive/paper-classification/data/arxiv/2002-5000.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH0cj1lsx4rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datalist = [pd.read_csv(f) for f in filelist]\n",
        "dataframe = pd.concat(datalist, axis=0, ignore_index=False)\n",
        "dataframe.to_csv(os.path.join(cwd, 'data/arxiv/{}.csv'.format(yymm)),\n",
        "                 index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBnjM_jkkpeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}