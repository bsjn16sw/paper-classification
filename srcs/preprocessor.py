# -*- coding: utf-8 -*-
"""preprocessor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zhSmUPHiWq1WYdFnsWwlkuIEtemtPBiw
"""

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import re

from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer

def preprocess(titles, texts):
    nltk.download('punkt')
    nltk.download('stopwords')
    
    prep_texts = []
    stop_words = set(stopwords.words('english'))

    for title, text in zip(titles, texts):
        sents = sent_tokenize(text)
        sents.append(title)
        sents = [re.sub(r"[^a-z0-9-]+", " ", sent.lower()) for sent in sents]

        cleaned_words = []
        for sent in sents:
            words = word_tokenize(sent)
            for word in words:
                if (word not in stop_words) and (len(word) > 2):
                    cleaned_words.append(word)
    
        prep_texts.append(cleaned_words)
    
    return prep_texts

def preprocess_lemma(titles, texts):
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('wordnet')
    
    prep_texts = []
    stop_words = set(stopwords.words('english'))

    for title, text in zip(titles, texts):
        sents = sent_tokenize(text)
        sents.append(title)
        sents = [re.sub(r"[^a-z0-9-]+", " ", sent.lower()) for sent in sents]

        cleaned_words = []
        for sent in sents:
            words = word_tokenize(sent)
            for word in words:
                if (word not in stop_words) and (len(word) > 2):
                    cleaned_words.append(word)
    
        prep_texts.append(cleaned_words)

    n = WordNetLemmatizer()
    lemm_texts = []
    for t in prep_texts:
        lemm_words = [n.lemmatize(w) for w in t]
        lemm_texts.append(lemm_words)
    
    return lemm_texts

def preprocess_pstem(titles, texts):
    nltk.download('punkt')
    nltk.download('stopwords')
    
    prep_texts = []
    stop_words = set(stopwords.words('english'))

    for title, text in zip(titles, texts):
        sents = sent_tokenize(text)
        sents.append(title)
        sents = [re.sub(r"[^a-z0-9-]+", " ", sent.lower()) for sent in sents]

        cleaned_words = []
        for sent in sents:
            words = word_tokenize(sent)
            for word in words:
                if (word not in stop_words) and (len(word) > 2):
                    cleaned_words.append(word)
    
        prep_texts.append(cleaned_words)

    p = PorterStemmer()
    stem_texts = []
    for t in prep_texts:
        stem_words = [p.stem(w) for w in t]
        stem_texts.append(stem_words)
    
    return stem_texts

def preprocess_lstem(titles, texts):
    nltk.download('punkt')
    nltk.download('stopwords')
    
    prep_texts = []
    stop_words = set(stopwords.words('english'))

    for title, text in zip(titles, texts):
        sents = sent_tokenize(text)
        sents.append(title)
        sents = [re.sub(r"[^a-z0-9-]+", " ", sent.lower()) for sent in sents]

        cleaned_words = []
        for sent in sents:
            words = word_tokenize(sent)
            for word in words:
                if (word not in stop_words) and (len(word) > 2):
                    cleaned_words.append(word)
    
        prep_texts.append(cleaned_words)

    l = LancasterStemmer()
    stem_texts = []
    for t in prep_texts:
        stem_words = [l.stem(w) for w in t]
        stem_texts.append(stem_words)
    
    return stem_texts